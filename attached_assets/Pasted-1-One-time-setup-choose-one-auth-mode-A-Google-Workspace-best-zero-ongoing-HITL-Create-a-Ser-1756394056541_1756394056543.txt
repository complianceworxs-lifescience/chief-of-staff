1) One-time setup (choose one auth mode)
A) Google Workspace (best: zero ongoing HITL)

Create a Service Account in Google Cloud, enable Gmail API, turn on Domain-Wide Delegation.

In Admin Console, grant the SA the scope:
https://www.googleapis.com/auth/gmail.readonly

Save the SA key JSON to a Replit Secret: GSA_JSON (entire JSON).

Add GMAIL_AUTH_MODE=service and GMAIL_IMPERSONATE=you@yourdomain.com.

B) Personal Gmail (one-time consent, then zero HITL)

Create OAuth Client (Desktop) in Google Cloud; enable Gmail API.

Run the standard quickstart locally once to get a refresh token.

Store in Replit Secrets: GMAIL_AUTH_MODE=oauth, OAUTH_CLIENT_ID, OAUTH_CLIENT_SECRET, OAUTH_REFRESH_TOKEN.

Daily runs are fully automated in both cases. Only the initial consent (Mode B) needs a human once.

2) Add deps (requirements.txt)
google-api-python-client>=2.141.0
google-auth>=2.33.0
google-auth-httplib2>=0.2.0
google-auth-oauthlib>=1.2.1
beautifulsoup4>=4.12.3
html2text>=2024.2.26
requests>=2.31.0

3) Drop-in Gmail puller (saves to /data)

Create gmail_pull.py:

# gmail_pull.py
import os, json, base64, email
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Any, Optional

from googleapiclient.discovery import build
from google.oauth2 import service_account
from google.oauth2.credentials import Credentials
from google.auth.transport.requests import Request

from bs4 import BeautifulSoup
import html2text

DATA_DIR = os.getenv("DATA_DIR", "data")
LABEL = os.getenv("GMAIL_LABEL", "cw/daily-reports")  # set this label in Gmail
QUERY = os.getenv("GMAIL_QUERY", f'label:"{LABEL}" newer_than:2d')

def _ensure_dirs():
    os.makedirs(DATA_DIR, exist_ok=True)
    os.makedirs(os.path.join(DATA_DIR, "inbox"), exist_ok=True)

def _save_json(name: str, obj: Any):
    _ensure_dirs()
    with open(os.path.join(DATA_DIR, name), "w", encoding="utf-8") as f:
        json.dump(obj, f, indent=2)

def _save_inbox(msg: Dict[str, Any], suffix="raw"):
    ts = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%SZ")
    safe_subj = "".join(c for c in msg.get("subject","") if c.isalnum() or c in (" ","-","_"))[:80].strip().replace(" ","_")
    fname = f"inbox/{ts}_{safe_subj}_{suffix}.json"
    _save_json(fname, msg)

def _gmail_service():
    mode = os.getenv("GMAIL_AUTH_MODE", "service").lower()
    if mode == "service":
        gsa_json = os.getenv("GSA_JSON")
        if not gsa_json: raise RuntimeError("Missing GSA_JSON secret")
        info = json.loads(gsa_json)
        creds = service_account.Credentials.from_service_account_info(
            info, scopes=["https://www.googleapis.com/auth/gmail.readonly"]
        )
        user = os.getenv("GMAIL_IMPERSONATE")
        if not user: raise RuntimeError("Missing GMAIL_IMPERSONATE")
        creds = creds.with_subject(user)
        return build("gmail", "v1", credentials=creds, cache_discovery=False)
    elif mode == "oauth":
        cid = os.getenv("OAUTH_CLIENT_ID"); cs = os.getenv("OAUTH_CLIENT_SECRET"); rt = os.getenv("OAUTH_REFRESH_TOKEN")
        if not all([cid, cs, rt]): raise RuntimeError("Missing OAuth secrets")
        creds = Credentials(None, refresh_token=rt, token_uri="https://oauth2.googleapis.com/token",
                            client_id=cid, client_secret=cs, scopes=["https://www.googleapis.com/auth/gmail.readonly"])
        if not creds.valid:
            creds.refresh(Request())
        return build("gmail", "v1", credentials=creds, cache_discovery=False)
    else:
        raise RuntimeError("GMAIL_AUTH_MODE must be 'service' or 'oauth'")

def _list_messages(svc, q: str, max_results=25) -> List[Dict[str, Any]]:
    res = svc.users().messages().list(userId="me", q=q, maxResults=max_results).execute()
    msgs = res.get("messages", [])
    out = []
    for m in msgs:
        out.append(svc.users().messages().get(userId="me", id=m["id"], format="full").execute())
    return out

def _parse_body(payload: Dict[str, Any]) -> Dict[str, str]:
    def walk(p):
        if "parts" in p:
            for part in p["parts"]:
                r = walk(part)
                if r: return r
        mime = p.get("mimeType","")
        body = p.get("body",{}).get("data")
        if body:
            raw = base64.urlsafe_b64decode(body.encode("utf-8"))
            if "text/html" in mime:
                soup = BeautifulSoup(raw, "html.parser")
                html = str(soup)
                text = html2text.html2text(html)
                return {"html": html, "text": text}
            elif "text/plain" in mime:
                return {"html": "", "text": raw.decode("utf-8", "ignore")}
        return None
    return walk(payload) or {"html":"", "text":""}

def _headers(msg) -> Dict[str,str]:
    h = {x["name"].lower(): x["value"] for x in msg.get("payload",{}).get("headers", [])}
    return {
        "from": h.get("from",""),
        "to": h.get("to",""),
        "subject": h.get("subject",""),
        "date": h.get("date","")
    }

# ----- mappers: adapt these 2–3 functions to your email formats ----------------

def map_ceo_to_scoreboard(text: str) -> Optional[Dict[str, Any]]:
    """Heuristic mapper for CEO summary → scoreboard.json. Customize as needed."""
    # Defaults
    out = {
        "date": datetime.now().date().isoformat(),
        "revenue": {"realized_week": 0, "target_week": 0, "upsells": 0},
        "initiatives": {"on_time_pct": 0, "risk_inverted": 0, "resource_ok_pct": 0, "dependency_clear_pct": 0},
        "alignment": {"work_tied_to_objectives_pct": 0},
        "autonomy": {"auto_resolve_pct": 0, "mttr_min": 0},
        "risk": {"score": 0, "high": 0, "medium": 0, "next_deadline_hours": 0},
        "narrative": {"topic": "", "linkedin_er_delta_pct": 0, "email_ctr_delta_pct": 0, "quiz_to_paid_delta_pct": 0, "conversions": 0}
    }
    import re
    m = re.search(r"Net New MRR[:\s]\$?([\d,]+)", text, re.I)
    # If you embed target/realized lines in email, extract them here. Otherwise leave to dashboards.
    if m:
        # Treat Net New MRR as upsells proxy (adjust if your email has better fields)
        out["revenue"]["upsells"] = float(m.group(1).replace(",",""))
    m = re.search(r"Autonomy[:\s]([\d\.]+)%", text, re.I)
    if m: out["autonomy"]["auto_resolve_pct"] = float(m.group(1))
    m = re.search(r"Quiz.?→.?Paid[:\s]([\d\.]+)%", text, re.I)
    if m: out["narrative"]["quiz_to_paid_delta_pct"] = float(m.group(1))
    return out

def map_content_to_actions(text: str) -> Dict[str, Any]:
    """Heuristic mapper for Content Digest → actions/meetings."""
    actions = []
    import re
    top = re.search(r'Top Piece[:\s]“?\"?(.+?)\"?[\r\n]', text)
    if top:
        actions.append({
            "title": f"Amplify: {top.group(1).strip()}",
            "owner": "CMO",
            "eta_days": 2,
            "reason": "Top-performing piece in Content Digest"
        })
    return {"actions": actions, "meetings": []}

# ----- main entry --------------------------------------------------------------

def pull_and_write():
    svc = _gmail_service()
    msgs = _list_messages(svc, QUERY, max_results=30)
    if not msgs:
        return

    ceo_scoreboard = None
    collected_actions = []
    collected_meetings = []

    for m in msgs:
        hdr = _headers(m)
        body = _parse_body(m.get("payload",{}))
        record = {"headers": hdr, "snippet": m.get("snippet",""), "body_text": body["text"][:5000]}
        _save_inbox({**record, "id": m.get("id")}, suffix="msg")

        subj = hdr["subject"].lower()
        if "ceo oversight" in subj or "ceo summary" in subj:
            ceo_scoreboard = map_ceo_to_scoreboard(body["text"])
        if "content digest" in subj or "content" in subj:
            mapped = map_content_to_actions(body["text"])
            collected_actions += mapped.get("actions", [])
            collected_meetings += mapped.get("meetings", [])

    # Write only if we actually mapped something; dashboards will still overwrite with fresher numbers
    if ceo_scoreboard:
        _save_json("scoreboard.json", ceo_scoreboard)
    if collected_actions:
        # Merge with existing actions.json if present
        path = os.path.join(DATA_DIR,"actions.json")
        if os.path.exists(path):
            with open(path,"r",encoding="utf-8") as f: existing = json.load(f)
        else:
            existing = []
        _save_json("actions.json", existing + collected_actions)

if __name__ == "__main__":
    pull_and_write()


What it does

Pulls labeled emails (last 2 days), extracts text/HTML, drops raw copies into /data/inbox/ for audit, and writes/merges scoreboard.json and actions.json when it finds matching subjects.

You can expand the two map_* functions to parse more metrics from your exact emails.

4) Call the puller before the morning run

In orchestrator.py, import and call it before validation/LLM:

# orchestrator.py (snippet)
from gmail_pull import pull_and_write

def morning_cycle():
    # 0) Pull emails first (06:30–06:34 window); safe to call again at 06:35
    try:
        pull_and_write()
    except Exception as e:
        print("[warn] gmail_pull failed:", e)

    # ... then load JSONs, validate, call LLM, policy-gate, dispatch ...


If you’re using the built-in time loop, either:

run pull_and_write() at 06:30 and morning_cycle() at 06:35, or

just call pull_and_write() at the start of morning_cycle() as above.

5) Gmail search you’ll likely want (set in env)
GMAIL_LABEL=cw/daily-reports
GMAIL_QUERY=label:"cw/daily-reports" newer_than:2d category:primary


(Use Gmail filters to auto-label your CEO & Content emails into cw/daily-reports.)

6) Privacy & safety (keeps you compliant)

Scope is read-only; we never modify Gmail.

We write aggregates into /data/*.json; raw emails are kept under /data/inbox/ for audit and can be purged daily.

Transport is TLS 1.2/1.3; keep Replit secrets off git.

The ODAR policy gate still runs after the model returns its plan.

TL;DR

Option B = pull, not push; no daily human needed.

Add the puller, set secrets, label your emails, call puller at 06:30–06:35.

The rest of your loop (validators → LLM → policy gate → dispatch) stays exactly the same.